"id","name","created_date","context_length","pricing_prompt","pricing_completion","response_time_seconds","tokens_used","performance_rank","description"
"ai21/jamba-mini-1.7","AI21: Jamba Mini 1.7","2025-08-09","256000","0.0000002","0.0000004","Jamba Mini 1.7 is a compact and efficient member of the Jamba open model family, incorporating key improvements in grounding and instruction-following while maintaining the benefits of the SSM-Transfo..."
"aion-labs/aion-1.0-mini","AionLabs: Aion-1.0-Mini","2025-02-05","131072","0.0000007","0.0000014","Aion-1.0-Mini 32B parameter model is a distilled version of the DeepSeek-R1 model, designed for strong performance in reasoning domains such as mathematics, coding, and logic. It is a modified variant..."
"alibaba/tongyi-deepresearch-30b-a3b","Tongyi DeepResearch 30B A3B","2025-09-19","131072","0.00000009","0.0000004","Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion total parameters activating only 3 billion per token. It's optimized for long-horizon, deep information-..."
"alibaba/tongyi-deepresearch-30b-a3b:free","Tongyi DeepResearch 30B A3B (free)","2025-09-19","131072","0","0","Tongyi DeepResearch is an agentic large language model developed by Tongyi Lab, with 30 billion total parameters activating only 3 billion per token. It's optimized for long-horizon, deep information-..."
"arcee-ai/maestro-reasoning","Arcee AI: Maestro Reasoning","2025-05-06","131072","0.0000009","0.0000033","Maestro Reasoning is Arcee's flagship analysis model: a 32 B‑parameter derivative of Qwen 2.5‑32 B tuned with DPO and chain‑of‑thought RL for step‑by‑step logic. Compared to the earlier 7 B preview, t..."
"arcee-ai/virtuoso-large","Arcee AI: Virtuoso Large","2025-05-06","131072","0.00000075","0.0000012","Virtuoso‑Large is Arcee's top‑tier general‑purpose LLM at 72 B parameters, tuned to tackle cross‑domain reasoning, creative writing and enterprise QA. Unlike many 70 B peers, it retains the 128 k cont..."
"baidu/ernie-4.5-21b-a3b","Baidu: ERNIE 4.5 21B A3B","2025-08-13","120000","0.00000007","0.00000028","A sophisticated text-based Mixture-of-Experts (MoE) model featuring 21B total parameters with 3B activated per token, delivering exceptional multimodal understanding and generation through heterogeneo..."
"baidu/ernie-4.5-21b-a3b-thinking","Baidu: ERNIE 4.5 21B A3B Thinking","2025-10-10","131072","0.00000007","0.00000028","ERNIE-4.5-21B-A3B-Thinking is Baidu's upgraded lightweight MoE model, refined to boost reasoning depth and quality for top-tier performance in logical puzzles, math, science, coding, text generation, ..."
"baidu/ernie-4.5-300b-a47b","Baidu: ERNIE 4.5 300B A47B ","2025-07-01","123000","0.00000028","0.0000011","ERNIE-4.5-300B-A47B is a 300B parameter Mixture-of-Experts (MoE) language model developed by Baidu as part of the ERNIE 4.5 series. It activates 47B parameters per token and supports text generation i..."
"deepseek/deepseek-chat-v3-0324","DeepSeek: DeepSeek V3 0324","2025-03-24","163840","0.00000024","0.00000084","DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.  It succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) mo..."
"deepseek/deepseek-chat-v3-0324:free","DeepSeek: DeepSeek V3 0324 (free)","2025-03-24","163840","0","0","DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.  It succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) mo..."
"deepseek/deepseek-chat-v3.1","DeepSeek: DeepSeek V3.1","2025-08-21","163840","0.0000002","0.0000008","DeepSeek-V3.1 is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes via prompt templates. It extends the DeepSeek-V3 base with a two-phase ..."
"deepseek/deepseek-prover-v2","DeepSeek: DeepSeek Prover V2","2025-04-30","163840","0.0000005","0.00000218","DeepSeek Prover V2 is a 671B parameter model, speculated to be geared towards logic and mathematics. Likely an upgrade from [DeepSeek-Prover-V1.5](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1..."
"deepseek/deepseek-r1","DeepSeek: R1","2025-01-20","163840","0.0000003","0.0000012","DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.  Fully ..."
"deepseek/deepseek-r1-0528","DeepSeek: R1 0528","2025-05-29","163840","0.0000002","0.0000045","May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in siz..."
"deepseek/deepseek-r1-0528-qwen3-8b:free","DeepSeek: DeepSeek R1 0528 Qwen3 8B (free)","2025-05-30","131072","0","0","DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and G..."
"deepseek/deepseek-r1-0528:free","DeepSeek: R1 0528 (free)","2025-05-29","163840","0","0","May 28th update to the [original DeepSeek R1](/deepseek/deepseek-r1) Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in siz..."
"deepseek/deepseek-r1-distill-llama-70b","DeepSeek: R1 Distill Llama 70B","2025-01-24","131072","0.00000003","0.00000013","DeepSeek R1 Distill Llama 70B is a distilled large language model based on [Llama-3.3-70B-Instruct](/meta-llama/llama-3.3-70b-instruct), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). The mo..."
"deepseek/deepseek-r1-distill-qwen-32b","DeepSeek: R1 Distill Qwen 32B","2025-01-30","131072","0.00000027","0.00000027","DeepSeek R1 Distill Qwen 32B is a distilled large language model based on [Qwen 2.5 32B](https://huggingface.co/Qwen/Qwen2.5-32B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperfor..."
"deepseek/deepseek-r1:free","DeepSeek: R1 (free)","2025-01-20","163840","0","0","DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass.  Fully ..."
"deepseek/deepseek-v3.1-terminus","DeepSeek: DeepSeek V3.1 Terminus","2025-09-22","163840","0.00000023","0.0000009","DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language cons..."
"deepseek/deepseek-v3.1-terminus:exacto","DeepSeek: DeepSeek V3.1 Terminus (exacto)","2025-09-22","131072","0.00000027","0.000001","DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language cons..."
"deepseek/deepseek-v3.2-exp","DeepSeek: DeepSeek V3.2 Exp","2025-09-29","163840","0.00000027","0.0000004","DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grai..."
"ibm-granite/granite-4.0-h-micro","IBM: Granite 4.0 Micro","2025-10-20","131000","0.000000017","0.00000011","Granite-4.0-H-Micro is a 3B parameter from the Granite 4 family of models. These models are the latest in a series of models released by IBM. They are fine-tuned for long context tool calling. "
"inception/mercury","Inception: Mercury","2025-06-27","128000","0.00000025","0.000001","Mercury is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like GPT-4.1 Nano and Clau..."
"inception/mercury-coder","Inception: Mercury Coder","2025-05-01","128000","0.00000025","0.000001","Mercury Coder is the first diffusion large language model (dLLM). Applying a breakthrough discrete diffusion approach, the model runs 5-10x faster than even speed optimized models like Claude 3.5 Haik..."
"kwaipilot/kat-coder-pro:free","Kwaipilot: KAT-Coder-Pro V1 (free)","2025-11-10","256000","0","0","KAT-Coder-Pro V1 is KwaiKAT's most advanced agentic coding model in the KAT-Coder series. Designed specifically for agentic coding tasks, it excels in real-world software engineering scenarios, achiev..."
"meituan/longcat-flash-chat","Meituan: LongCat Flash Chat","2025-09-10","131072","0.00000015","0.00000075","LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of which 18.6B–31.3B (≈27B on average) are dynamically activated per input. It introduces a shortcut-conn..."
"meituan/longcat-flash-chat:free","Meituan: LongCat Flash Chat (free)","2025-09-10","131072","0","0","LongCat-Flash-Chat is a large-scale Mixture-of-Experts (MoE) model with 560B total parameters, of which 18.6B–31.3B (≈27B on average) are dynamically activated per input. It introduces a shortcut-conn..."
"meta-llama/llama-guard-3-8b","Llama Guard 3 8B","2025-02-13","131072","0.00000002","0.00000006","Llama Guard 3 is a Llama-3.1-8B pretrained model, fine-tuned for content safety classification. Similar to previous versions, it can be used to classify content in both LLM inputs (prompt classificati..."
"microsoft/mai-ds-r1","Microsoft: MAI DS R1","2025-04-21","163840","0.0000003","0.0000012","MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model’s responsiveness on previously blocked topics while enhancing its safety profile. Built on to..."
"microsoft/mai-ds-r1:free","Microsoft: MAI DS R1 (free)","2025-04-21","163840","0","0","MAI-DS-R1 is a post-trained variant of DeepSeek-R1 developed by the Microsoft AI team to improve the model’s responsiveness on previously blocked topics while enhancing its safety profile. Built on to..."
"minimax/minimax-m1","MiniMax: MiniMax M1","2025-06-18","1000000","0.0000004","0.0000022","MiniMax-M1 is a large-scale, open-weight reasoning model designed for extended context and high-efficiency inference. It leverages a hybrid Mixture-of-Experts (MoE) architecture paired with a custom ""..."
"minimax/minimax-m2","MiniMax: MiniMax M2","2025-10-24","204800","0.000000255","0.00000102","MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and agentic workflows. With 10 billion activated parameters (230 billion total), it delivers near-frontier..."
"mistralai/codestral-2501","Mistral: Codestral 2501","2025-01-15","256000","0.0000003","0.0000009","[Mistral](/mistralai)'s cutting-edge language model for coding. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test generation.   Lear..."
"mistralai/codestral-2508","Mistral: Codestral 2508","2025-08-02","256000","0.0000003","0.0000009","Mistral's cutting-edge language model for coding released end of July 2025. Codestral specializes in low-latency, high-frequency tasks such as fill-in-the-middle (FIM), code correction and test genera..."
"mistralai/devstral-medium","Mistral: Devstral Medium","2025-07-11","131072","0.0000004","0.000002","Devstral Medium is a high-performance code generation and agentic reasoning model developed jointly by Mistral AI and All Hands AI. Positioned as a step up from Devstral Small, it achieves 61.6% on SW..."
"mistralai/devstral-small","Mistral: Devstral Small 1.1","2025-07-11","128000","0.00000007","0.00000028","Devstral Small 1.1 is a 24B parameter open-weight language model for software engineering agents, developed by Mistral AI in collaboration with All Hands AI. Finetuned from Mistral Small 3.1 and relea..."
"mistralai/devstral-small-2505","Mistral: Devstral Small 2505","2025-05-22","128000","0.00000006","0.00000012","Devstral-Small-2505 is a 24B parameter agentic LLM fine-tuned from Mistral-Small-3.1, jointly developed by Mistral AI and All Hands AI for advanced software engineering tasks. It is optimized for code..."
"moonshotai/kimi-dev-72b","MoonshotAI: Kimi Dev 72B","2025-06-17","131072","0.00000029","0.00000115","Kimi-Dev-72B is an open-source large language model fine-tuned for software engineering and issue resolution tasks. Based on Qwen2.5-72B, it is optimized using large-scale reinforcement learning that ..."
"moonshotai/kimi-k2","MoonshotAI: Kimi K2 0711","2025-07-12","131072","0.0000005","0.0000024","Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for a..."
"moonshotai/kimi-k2-0905","MoonshotAI: Kimi K2 0905","2025-09-05","262144","0.00000039","0.0000019","Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters ..."
"moonshotai/kimi-k2-0905:exacto","MoonshotAI: Kimi K2 0905 (exacto)","2025-09-05","262144","0.0000006","0.0000025","Kimi K2 0905 is the September update of [Kimi K2 0711](moonshotai/kimi-k2). It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters ..."
"moonshotai/kimi-k2-thinking","MoonshotAI: Kimi K2 Thinking","2025-11-07","262144","0.00000045","0.00000235","Kimi K2 Thinking is Moonshot AI’s most advanced open reasoning model to date, extending the K2 series into agentic, long-horizon reasoning. Built on the trillion-parameter Mixture-of-Experts (MoE) arc..."
"moonshotai/kimi-linear-48b-a3b-instruct","MoonshotAI: Kimi Linear 48B A3B Instruct","2025-11-08","1048576","0.0000005","0.0000006","Kimi Linear is a hybrid linear attention architecture that outperforms traditional full attention methods across various contexts, including short, long, and reinforcement learning (RL) scaling regime..."
"morph/morph-v3-fast","Morph: Morph V3 Fast","2025-07-08","81920","0.0000008","0.0000012","Morph's fastest apply model for code edits. ~10,500 tokens/sec with 96% accuracy for rapid code transformations.  The model requires the prompt to be in the following format:  <instruction>{instructio..."
"morph/morph-v3-large","Morph: Morph V3 Large","2025-07-08","262144","0.0000009","0.0000019","Morph's high-accuracy apply model for complex code edits. ~4,500 tokens/sec with 98% accuracy for precise code transformations.  The model requires the prompt to be in the following format:  <instruct..."
"nousresearch/hermes-4-405b","Nous: Hermes 4 405B","2025-08-27","131072","0.0000003","0.0000012","Hermes 4 is a large-scale reasoning model built on Meta-Llama-3.1-405B and released by Nous Research. It introduces a hybrid reasoning mode, where the model can choose to deliberate internally with <t..."
"nousresearch/hermes-4-70b","Nous: Hermes 4 70B","2025-08-27","131072","0.00000011","0.00000038","Hermes 4 70B is a hybrid reasoning model from Nous Research, built on Meta-Llama-3.1-70B. It introduces the same hybrid mode as the larger 405B release, allowing the model to either respond directly o..."
"nvidia/llama-3.1-nemotron-ultra-253b-v1","NVIDIA: Llama 3.1 Nemotron Ultra 253B v1","2025-04-08","131072","0.0000006","0.0000018","Llama-3.1-Nemotron-Ultra-253B-v1 is a large language model (LLM) optimized for advanced reasoning, human-interactive chat, retrieval-augmented generation (RAG), and tool-calling tasks. Derived from Me..."
"nvidia/llama-3.3-nemotron-super-49b-v1.5","NVIDIA: Llama 3.3 Nemotron Super 49B V1.5","2025-10-10","131072","0.0000001","0.0000004","Llama-3.3-Nemotron-Super-49B-v1.5 is a 49B-parameter, English-centric reasoning/chat model derived from Meta’s Llama-3.3-70B-Instruct with a 128K context. It’s post-trained for agentic workflows (RAG,..."
"nvidia/nemotron-nano-9b-v2","NVIDIA: Nemotron Nano 9B V2","2025-09-06","131072","0.00000004","0.00000016","NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and t..."
"openai/gpt-oss-120b","OpenAI: gpt-oss-120b","2025-08-06","131072","0.00000004","0.0000004","gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B par..."
"openai/gpt-oss-120b:exacto","OpenAI: gpt-oss-120b (exacto)","2025-08-06","131072","0.00000005","0.00000024","gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B par..."
"openai/gpt-oss-20b","OpenAI: gpt-oss-20b","2025-08-06","131072","0.00000003","0.00000014","gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimiz..."
"openai/gpt-oss-20b:free","OpenAI: gpt-oss-20b (free)","2025-08-06","131072","0","0","gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimiz..."
"openai/gpt-oss-safeguard-20b","OpenAI: gpt-oss-safeguard-20b","2025-10-30","131072","0.000000075","0.0000003","gpt-oss-safeguard-20b is a safety reasoning model from OpenAI built upon gpt-oss-20b. This open-weight, 21B-parameter Mixture-of-Experts (MoE) model offers lower latency for safety tasks like content ..."
"qwen/qwen-plus","Qwen: Qwen-Plus","2025-02-01","131072","0.0000004","0.0000012","Qwen-Plus, based on the Qwen2.5 foundation model, is a 131K context model with a balanced performance, speed, and cost combination."
"qwen/qwen-plus-2025-07-28","Qwen: Qwen Plus 0728","2025-09-09","1000000","0.0000004","0.0000012","Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination."
"qwen/qwen-plus-2025-07-28:thinking","Qwen: Qwen Plus 0728 (thinking)","2025-09-09","1000000","0.0000004","0.000004","Qwen Plus 0728, based on the Qwen3 foundation model, is a 1 million context hybrid reasoning model with a balanced performance, speed, and cost combination."
"qwen/qwen-turbo","Qwen: Qwen-Turbo","2025-02-01","1000000","0.00000005","0.0000002","Qwen-Turbo, based on Qwen2.5, is a 1M context model that provides fast speed and low cost, suitable for simple tasks."
"qwen/qwen3-235b-a22b-2507","Qwen: Qwen3 235B A22B Instruct 2507","2025-07-22","262144","0.00000008","0.00000055","Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized ..."
"qwen/qwen3-235b-a22b-thinking-2507","Qwen: Qwen3 235B A22B Thinking 2507","2025-07-25","262144","0.00000011","0.0000006","Qwen3-235B-A22B-Thinking-2507 is a high-performance, open-weight Mixture-of-Experts (MoE) language model optimized for complex reasoning tasks. It activates 22B of its 235B parameters per forward pass..."
"qwen/qwen3-30b-a3b-instruct-2507","Qwen: Qwen3 30B A3B Instruct 2507","2025-07-30","262144","0.00000008","0.00000033","Qwen3-30B-A3B-Instruct-2507 is a 30.5B-parameter mixture-of-experts language model from Qwen, with 3.3B active parameters per inference. It operates in non-thinking mode and is designed for high-quali..."
"qwen/qwen3-8b","Qwen: Qwen3 8B","2025-04-29","128000","0.000000035","0.000000138","Qwen3-8B is a dense 8.2B parameter causal language model from the Qwen3 series, designed for both reasoning-heavy tasks and efficient dialogue. It supports seamless switching between ""thinking"" mode f..."
"qwen/qwen3-coder","Qwen: Qwen3 Coder 480B A35B","2025-07-23","262144","0.00000022","0.00000095","Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-con..."
"qwen/qwen3-coder-30b-a3b-instruct","Qwen: Qwen3 Coder 30B A3B Instruct","2025-08-01","262144","0.00000006","0.00000025","Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, an..."
"qwen/qwen3-coder-flash","Qwen: Qwen3 Coder Flash","2025-09-17","128000","0.0000003","0.0000015","Qwen3 Coder Flash is Alibaba's fast and cost efficient version of their proprietary Qwen3 Coder Plus. It is a powerful coding agent model specializing in autonomous programming via tool calling and en..."
"qwen/qwen3-coder:exacto","Qwen: Qwen3 Coder 480B A35B (exacto)","2025-07-23","262144","0.00000038","0.00000153","Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-con..."
"qwen/qwen3-coder:free","Qwen: Qwen3 Coder 480B A35B (free)","2025-07-23","262000","0","0","Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-con..."
"qwen/qwen3-next-80b-a3b-instruct","Qwen: Qwen3 Next 80B A3B Instruct","2025-09-12","262144","0.0000001","0.0000008","Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model in the Qwen3-Next series optimized for fast, stable responses without “thinking” traces. It targets complex tasks across reasoning, code ..."
"qwen/qwen3-next-80b-a3b-thinking","Qwen: Qwen3 Next 80B A3B Thinking","2025-09-12","262144","0.00000015","0.0000012","Qwen3-Next-80B-A3B-Thinking is a reasoning-first chat model in the Qwen3-Next line that outputs structured “thinking” traces by default. It’s designed for hard multi-step problems; math proofs, code s..."
"switchpoint/router","Switchpoint Router","2025-07-12","131072","0.00000085","0.0000034","Switchpoint AI's router instantly analyzes your request and directs it to the optimal AI from an ever-evolving library.   As the world of LLMs advances, our router gets smarter, ensuring you always be..."
"tencent/hunyuan-a13b-instruct","Tencent: Hunyuan A13B Instruct","2025-07-09","131072","0.00000014","0.00000057","Hunyuan-A13B is a 13B active parameter Mixture-of-Experts (MoE) language model developed by Tencent, with a total parameter count of 80B and support for reasoning via Chain-of-Thought. It offers compe..."
"thedrummer/anubis-70b-v1.1","TheDrummer: Anubis 70B V1.1","2025-06-30","131072","0.00000065","0.000001","TheDrummer's Anubis v1.1 is an unaligned, creative Llama 3.3 70B model focused on providing character-driven roleplay & stories. It excels at gritty, visceral prose, unique character adherence, and co..."
"thedrummer/cydonia-24b-v4.1","TheDrummer: Cydonia 24B V4.1","2025-09-27","131072","0.0000003","0.0000005","Uncensored and creative writing model based on Mistral Small 3.2 24B with good recall, prompt adherence, and intelligence."
"tngtech/deepseek-r1t-chimera","TNG: DeepSeek R1T Chimera","2025-04-27","163840","0.0000003","0.0000012","DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Tr..."
"tngtech/deepseek-r1t-chimera:free","TNG: DeepSeek R1T Chimera (free)","2025-04-27","163840","0","0","DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Tr..."
"tngtech/deepseek-r1t2-chimera","TNG: DeepSeek R1T2 Chimera","2025-07-09","163840","0.0000003","0.0000012","DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 c..."
"tngtech/deepseek-r1t2-chimera:free","TNG: DeepSeek R1T2 Chimera (free)","2025-07-09","163840","0","0","DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 c..."
"x-ai/grok-3-mini","xAI: Grok 3 Mini","2025-06-11","131072","0.0000003","0.0000005","A lightweight model that thinks before responding. Fast, smart, and great for logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible."
"x-ai/grok-3-mini-beta","xAI: Grok 3 Mini Beta","2025-04-10","131072","0.0000003","0.0000005","Grok 3 Mini is a lightweight, smaller thinking model. Unlike traditional models that generate answers immediately, Grok 3 Mini thinks before responding. It’s ideal for reasoning-heavy tasks that don’t..."
"x-ai/grok-code-fast-1","xAI: Grok Code Fast 1","2025-08-27","256000","0.0000002","0.0000015","Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows."
"z-ai/glm-4-32b","Z.AI: GLM 4 32B ","2025-07-25","128000","0.0000001","0.0000001","GLM 4 32B is a cost-effective foundation language model.  It can efficiently perform complex tasks and has significantly enhanced capabilities in tool use, online search, and code-related intelligent ..."
"z-ai/glm-4.5","Z.AI: GLM 4.5","2025-07-26","131072","0.00000035","0.0000015","GLM-4.5 is our latest flagship foundation model, purpose-built for agent-based applications. It leverages a Mixture-of-Experts (MoE) architecture and supports a context length of up to 128k tokens. GL..."
"z-ai/glm-4.5-air","Z.AI: GLM 4.5 Air","2025-07-26","131072","0.00000013","0.00000085","GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but wit..."
"z-ai/glm-4.5-air:free","Z.AI: GLM 4.5 Air (free)","2025-07-26","131072","0","0","GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but wit..."
"z-ai/glm-4.6","Z.AI: GLM 4.6","2025-09-30","202752","0.0000004","0.00000175","Compared with GLM-4.5, this generation brings several key improvements:  Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex..."
"z-ai/glm-4.6:exacto","Z.AI: GLM 4.6 (exacto)","2025-09-30","202752","0.00000045","0.0000019","Compared with GLM-4.5, this generation brings several key improvements:  Longer context window: The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex..."
